---
title: "Significance Testing of Differences Between Predictions: Slopes, Contrasts and Pairwise Comparisons"
author: "Daniel Lüdecke"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Significance Testing of Differences Between Predictions: Slopes, Contrasts and Pairwise Comparisons}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r set-options, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dev = "png",
  fig.width = 7,
  fig.height = 3.5,
  message = FALSE, warning = FALSE)
options(width = 800)
arrow_color <- "#FF00cc"

if (!requireNamespace("ggplot2", quietly = TRUE) ||
    !requireNamespace("marginaleffects", quietly = TRUE) ||
    !requireNamespace("parameters", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}
```

# Hypothesis testing for categorical predictors

A reason to compute adjusted predictions (or estimated marginal means) is to help understanding the relationship between predictors and outcome of a regression model. In particular for more complex models, for example, complex interaction terms, it is often easier to understand the associations when looking at adjusted predictions instead of the raw table of regression coefficients.

The next step, which often follows this, is to see if there are statistically significant differences. These could be, for example, differences between groups, i.e. between the levels of categorical predictors or whether trends differ significantly from each other.

The *ggeffects* package provides a function, `hypothesis_test()`, which does exactly this: testing differences of adjusted predictions for statistical significance. This is usually called _contrasts_ or _(pairwise) comparisons_. This vignette shows some examples how to use the `hypothesis_test()` function and how to test wheter differences in predictions are statistically significant.

## Within `episode`, do levels differ?

We start with a toy example, where we have a linear model with two categorical predictors. No interaction is involved for now.

We display a simple table of regression coefficients, created with `model_parameters()` from the _parameters_ package.

```{r}
library(ggeffects)
library(parameters)
library(ggplot2)

set.seed(123)
n <- 200
d <- data.frame(
  outcome = rnorm(n),
  grp = as.factor(sample(c("treatment", "control"), n, TRUE)),
  episode = as.factor(sample(1:3, n, TRUE)),
  sex = as.factor(sample(c("female", "male"), n, TRUE, prob = c(0.4, 0.6)))
)
model1 <- lm(outcome ~ grp + episode, data = d)
model_parameters(model1)
```

### Predictions

Let us look at the adjusted predictions.

```{r}
mydf <- ggpredict(model1, "episode")
mydf

plot(mydf)
```

We now see that, for instance, the predicted _outcome_ when `espisode = 2` is `r round(mydf$predicted[2], 2)`.

### Pairwise comparisons

We could now ask whether the predicted outcome for `episode = 1` is significantly different from the predicted outcome at `episode = 2`.

```{r echo=FALSE}
p <- plot(mydf)
line_data <- as.data.frame(mydf, terms_to_colnames = FALSE)[1:2, ]
p + geom_segment(
  data = line_data,
  aes(
    x = as.numeric(x[1]) + 0.06, xend = as.numeric(x[2]) - 0.06,
    y = predicted[1], yend = predicted[2], group = NULL, color = NULL
  ),
  color = arrow_color,
  arrow = arrow(length = unit(0.1, "inches"), ends = "both", angle = 40)
) +
ggtitle("Within \"episode\", do levels 1 and 2 differ?")
ht1 <- hypothesis_test(model1, "episode")
```

To do this, we use the `hypothesis_test()` function. This function, like `ggpredict()`, accepts the model object as first argument, followed by the _focal predictors_ of interest, i.e. the variables of the model for which contrasts or pairwise comparisons should be calculated.

By default, when all focal terms are categorical, a pairwise comparison is performed. You can specify other hypothesis tests as well, using the `test` argument (which defaults to `"pairwise"`, see `?hypothesis_test`). For now, we go on with the simpler example of contrasts or pairwise comparisons.

```{r}
hypothesis_test(model1, "episode") # argument `test` defaults to "pairwise"
```

For our quantity of interest, the contrast between episode `r ht1$episode[1]`, we see the value `r round(ht1$Contrast[1], 2)`, which is exactly the difference between the predicted outcome for `episode = 1` (`r round(mydf$predicted[1], 2)`) and `episode = 2` (`r round(mydf$predicted[2], 2)`). The related p-value is `r round(ht1$p.value[1], 3)`, indicating that the difference between the predicted values of our outcome at these two levels of the factor _episode_ is indeed statistically significant.

In this simple example, the contrasts of both `episode = 2` and `episode = 3` to `episode = 1` equals the coefficients of the regression table above (same applies to the p-values), where the coefficients refer to the difference between the related parameter of `episode` and its reference level, `episode = 1`.

To avoid specifying all arguments used in a call to `ggpredict()` again, we can also pass the objects returned by `ggpredict()` directly into `hypothesis_test()`.

```{r}
pred <- ggpredict(model1, "episode")
hypothesis_test(pred)
```

## Does same level of episode differ between groups?

The next example includes a pairwise comparison of an interaction between two categorical predictors.

```{r}
model2 <- lm(outcome ~ grp * episode, data = d)
model_parameters(model2)
```

### Predictions

First, we look at the predicted values of _outcome_ for all combinations of the involved interaction term.

```{r}
mydf <- ggpredict(model2, c("episode", "grp"))
mydf

plot(mydf)
```

### Pairwise comparisons

We could now ask whether the predicted outcome for `episode = 2` is significantly different depending on the level of `grp`? In other words, do the groups `treatment` and `control` differ when `episode = 2`?

```{r echo=FALSE}
p <- plot(mydf)
line_data <- as.data.frame(mydf, terms_to_colnames = FALSE)[3:4, 1:2]
line_data$group_col <- "control"
p + geom_segment(
  data = line_data,
  aes(
    x = as.numeric(x[1]) - 0.06, xend = as.numeric(x[2]) + 0.06,
    y = predicted[1], yend = predicted[2], group = NULL, color = NULL
  ),
  color = arrow_color,
  arrow = arrow(length = unit(0.1, "inches"), ends = "both", angle = 40)
) +
ggtitle("Within level 2 of \"episode\", do treatment and control group differ?")
ht2 <- hypothesis_test(model2, c("episode", "grp"))
```

Again, to answer this question, we calculate all pairwise comparisons, i.e. the comparison (or test for differences) between all combinations of our _focal predictors_. The focal predictors we're interested here are our two variables used for the interaction.

```{r}
# we want "episode = 2-2" and "grp = control-treatment"
hypothesis_test(model2, c("episode", "grp"))
```

For our quantity of interest, the contrast between groups `treatment` and `control` when `episode = 2` is `r round(ht2$Contrast[8], 2)`. We find this comparison in row 8 of the above output.

As we can see, `hypothesis_test()` returns pairwise comparisons of all possible combinations of factor levels from our focal variables. If we're only interested in a very specific comparison, we could directly formulate this comparison as `test`. To achieve this, we first need to create an overview of the adjusted predictions, which we get from `ggpredict()` or `hypothesis_test(test = NULL)`.

```{r}
# adjusted predictions, formatted table
ggpredict(model2, c("episode", "grp"))

# adjusted predictions, compact table
hypothesis_test(model2, c("episode", "grp"), test = NULL)
```

In the above output, each row is considered as one coefficient of interest. Our groups we want to include in our comparison are rows two (`grp = control` and `episode = 2`) and five (`grp = treatment` and `episode = 2`), so our "quantities of interest" are `b2` and `b5`. Our null hypothesis we want to test is whether both predictions are equal, i.e. `test = "b2 = b5"`. We can now calculate the desired comparison directly:

```{r}
# compute specific contrast directly
hypothesis_test(model2, c("episode", "grp"), test = "b2 = b5")
```

The reason for this specific way of specifying the `test` argument is because `hypothesis_test()` is a small, convenient wrapper around `predictions()` and `slopes()` of the great [*marginaleffects*](https://vincentarelbundock.github.io/marginaleffects/) package. Thus, `test` is just passed to the [`hypothesis` argument](https://vincentarelbundock.github.io/marginaleffects/articles/hypothesis.html) of those functions.

## Do different episode levels differ between groups?

We can repeat the steps shown above to test any combination of group levels for differences.

### Pairwise comparisons

For instance, we could now ask whether the predicted outcome for `episode = 1` in the `treatment` group is significantly different from the predicted outcome for `episode = 3` in the `control` group.

```{r echo=FALSE}
p <- plot(mydf)
line_data <- as.data.frame(mydf, terms_to_colnames = FALSE)[c(2, 5), 1:2]
line_data$group_col <- "treatment"
p + geom_segment(
  data = line_data,
  aes(
    x = as.numeric(x[1]) + 0.06, xend = as.numeric(x[2]) - 0.06,
    y = predicted[1], yend = predicted[2], group = NULL, color = NULL
  ),
  color = arrow_color,
  arrow = arrow(length = unit(0.1, "inches"), ends = "both", angle = 40)
) +
ggtitle("Do different episode levels differ between groups?")
ht3 <- hypothesis_test(model2, c("episode", "grp"))
```

The contrast we are interested in is between `episode = 1` in the `treatment` group and `episode = 3` in the `control` group. These are the predicted values in rows three and four (c.f. above table of predicted values), thus we `test` whether `"b4 = b3"`.

```{r}
hypothesis_test(model2, c("episode", "grp"), test = "b4 = b3")
```

Another way to produce this pairwise comparison, we can reduce the table of predicted values by providing [specific values or levels](https://strengejacke.github.io/ggeffects/articles/introduction_effectsatvalues.html) in the `terms` argument:

```{r}
ggpredict(model2, c("episode [1,3]", "grp"))
```

`episode = 1` in the `treatment` group and `episode = 3` in the `control` group refer now to rows two and three, thus we also can obtain the desired comparison this way:

```{r}
pred <- ggpredict(model2, c("episode [1,3]", "grp"))
hypothesis_test(pred, test = "b3 = b2")
```

## Does difference between two levels of episode in the control group differ from difference of same two levels in the treatment group?

The `test` argument also allows us to compare difference-in-differences. For example, is the difference between two episode levels in one group significantly different from the difference of the same two episode levels in the other group?

```{r echo=FALSE}
mydf <- ggpredict(model2, c("grp", "episode"))
p <- plot(mydf)
line_data <- as.data.frame(mydf, terms_to_colnames = FALSE)[, 1:2, ]
line_data$group_col <- "1"
p + geom_segment(
  data = line_data,
  aes(
    x = as.numeric(x[1]) - 0.05, xend = as.numeric(x[1]) - 0.05,
    y = predicted[1], yend = predicted[2], group = NULL, color = NULL
  ),
  color = "orange",
  arrow = arrow(length = unit(0.1, "inches"), ends = "both", angle = 40, type = "closed")
) + geom_segment(
  data = line_data,
  aes(
    x = as.numeric(x[4]) - 0.05, xend = as.numeric(x[4]) - 0.05,
    y = predicted[4], yend = predicted[5], group = NULL, color = NULL
  ),
  color = "orange",
  arrow = arrow(length = unit(0.1, "inches"), ends = "both", angle = 40, type = "closed")
) + geom_segment(
  data = line_data,
  aes(
    x = as.numeric(x[1]) - 0.05, xend = as.numeric(x[4]) - 0.05,
    y = (predicted[1] + predicted[2]) / 2,
    yend = (predicted[4] + predicted[5]) / 2, group = NULL, color = NULL
  ),
  color = arrow_color,
  arrow = arrow(length = unit(0.1, "inches"), ends = "both", angle = 40)
) +
ggtitle("Differnce-in-differences")

ht4 <- hypothesis_test(model2, c("episode", "grp"), test = NULL)
ht5 <- hypothesis_test(model2, c("episode", "grp"), test = "(b1 - b2) = (b4 - b5)")
```

As a reminder, we look at the table of predictions again:

```{r}
hypothesis_test(model2, c("episode", "grp"), test = NULL)
```

The first difference of episode levels 1 and 2 in the control group refer to rows one and two in the above table (`b1` and `b2`). The difference for the same episode levels in the treatment group refer to the difference between rows four and five (`b4` and `b5`). Thus, we have `b1 - b2` and `b4 - b5`, and our null hypothesis is that these two differences are equal: `test = "(b1 - b2) = (b4 - b5)"`.

```{r}
hypothesis_test(model2, c("episode", "grp"), test = "(b1 - b2) = (b4 - b5)")
```

Let's replicate this step-by-step:

1. Predicted value of _outcome_ for `episode = 1` in the control group is `r round(ht4$Predicted[1], 2)`.
2. Predicted value of _outcome_ for `episode = 2` in the control group is `r round(ht4$Predicted[2], 2)`.
3. The first difference is `r round(ht4$Predicted[1] - ht4$Predicted[2], 2)`
4. Predicted value of _outcome_ for `episode = 1` in the treatment group is `r round(ht4$Predicted[4], 2)`.
5. Predicted value of _outcome_ for `episode = 2` in the treatment group is `r round(ht4$Predicted[5], 2)`.
6. The second difference is `r round(ht4$Predicted[4] - ht4$Predicted[5], 2)`
7. Our quantity of interest is the difference between these two differences, which is `r round((ht4$Predicted[1] - ht4$Predicted[2]) - (ht4$Predicted[4] - ht4$Predicted[5]), 2)`. This difference is not statistically significant (p = `r round(ht5$p.value, 3)`).

# Hypothesis testing for slopes of numeric predictors

For numeric focal terms, it is possible to conduct hypothesis testing for slopes, or the _linear trend_ of these focal terms.

Let's start with a simple example again.

```{r}
data(iris)
m <- lm(Sepal.Width ~ Sepal.Length + Species, data = iris)
model_parameters(m)
```

We can already see from the coefficient table that the slope for `Sepal.Length` is `r unname(round(coef(m)[2], 3))`. We will thus find the same increase for the predicted values in our outcome when our focal variable, `Sepal.Length` increases by one unit.

```{r}
ggpredict(m, "Sepal.Length [4,5,6,7]")
```

Consequently, in this case of a simple slope, we see the same result for the hypothesis test for the linar trend of `Sepal.Length`:

```{r}
hypothesis_test(m, "Sepal.Length")
```

## Is the linear trend of `Sepal.Length` significant for the different levels of `Species`?

Let's move on to a more complex example with an interaction between a numeric and categorical variable.

### Predictions

```{r}
m <- lm(Sepal.Width ~ Sepal.Length * Species, data = iris)
pred <- ggpredict(m, c("Sepal.Length", "Species"))
plot(pred)
```

### Slopes by group

We can see that the slope of `Sepal.Length` is different within each group of `Species`.

```{r echo=FALSE}
p <- plot(pred)
dat <- as.data.frame(pred, terms_to_colnames = FALSE)
dat1 <- data.frame(
  x = dat$x[c(13, 28)],
  y = dat$predicted[c(13, 28)],
  group_col = "setosa",
  stringsAsFactors = FALSE
)
dat2 <- data.frame(
  x = dat$x[c(8, 23)],
  y = dat$predicted[c(8, 23)],
  group_col = "versicolor",
  stringsAsFactors = FALSE
)
dat3 <- data.frame(
  x = dat$x[c(39, 54)],
  y = dat$predicted[c(39, 54)],
  group_col = "versicolor",
  stringsAsFactors = FALSE
)
p + geom_segment(
  data = dat1,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[1]),
  color = "orange", linewidth = 1, linetype = "dashed"
) + geom_segment(
  data = dat1,
  mapping = aes(x = x[2], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 1, linetype = "dashed"
) + geom_segment(
  data = dat1,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 1
)  + geom_segment(
  data = dat2,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[1]),
  color = "orange", linewidth = 1, linetype = "dashed"
) + geom_segment(
  data = dat2,
  mapping = aes(x = x[2], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 1, linetype = "dashed"
)  + geom_segment(
  data = dat2,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 1
)  + geom_segment(
  data = dat3,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[1]),
  color = "orange", linewidth = 1, linetype = "dashed"
) + geom_segment(
  data = dat3,
  mapping = aes(x = x[2], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 1, linetype = "dashed"
)  + geom_segment(
  data = dat3,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 1
) +
ggtitle("Linear trend of `Sepal.Length` by `Species`")
```

Since we don't want to do pairwise comparisons, we set `test = NULL`. In this case, when interaction terms are included, the linear trend (_slope_) for our numeric focal predictor, `Sepal.Length`, is tested for each level of `Species`.

```{r}
hypothesis_test(m, c("Sepal.Length", "Species"), test = NULL)
```

As we can see, each of the three slopes is significant, i.e. we have "significant" linear trends.

### Pairwise comparisons

Next question could be whether or not linear trends differ significantly between each other, i.e. we test differences in slopes, which is a pairwise comparison between slopes. To do this, we use the default for `test`, which is `"pairwise"`.

```{r}
hypothesis_test(m, c("Sepal.Length", "Species"))
```

```{r echo=FALSE}
ht5 <- hypothesis_test(m, c("Sepal.Length", "Species"))
```

The linear trend of `Sepal.Length` within `setosa` is significantly different from the linear trend of `versicolor` and also from `virginica`. The difference of slopes between `virginica` and `versicolor` is not statistically significant (p = `r round(ht5$p.value[3], 3)`).

## Is the difference linear trends of `Sepal.Length` in between two groups of `Species` significantly different from the difference of two linear trends between two other groups?

Similar to the example for categorical predictors, we can also test a difference-in-differences for this example. For instance, is the difference of the slopes from `Sepal.Length` between `setosa` and `versicolor` different from the slope-difference for the groups `setosa` and `vigninica`?

This difference-in-differences we're interested in is again indicated by the purple arrow in the below plot.

```{r echo=FALSE}
p <- plot(pred)
dat <- as.data.frame(pred, terms_to_colnames = FALSE)
dat1 <- data.frame(
  x = dat$x[c(10, 16)],
  y = dat$predicted[c(10, 16)],
  group_col = "setosa",
  stringsAsFactors = FALSE
)
dat2 <- data.frame(
  x = dat$x[c(11, 17)],
  y = dat$predicted[c(11, 17)],
  group_col = "versicolor",
  stringsAsFactors = FALSE
)
dat3 <- data.frame(
  x = dat$x[c(40, 46)],
  y = dat$predicted[c(40, 46)],
  group_col = "versicolor",
  stringsAsFactors = FALSE
)
dat4 <- data.frame(
  x = dat$x[c(42, 48)],
  y = dat$predicted[c(42, 48)],
  group_col = "versicolor",
  stringsAsFactors = FALSE
)
dat5 <- data.frame(
  x = dat$x[c(13, 14)],
  y = dat$predicted[c(13, 14)],
  group_col = "versicolor",
  stringsAsFactors = FALSE
)
dat6 <- data.frame(
  x = dat$x[c(43, 45)],
  y = dat$predicted[c(43, 45)],
  group_col = "versicolor",
  stringsAsFactors = FALSE
)
p + geom_segment(
  data = dat1,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 0.8
) + geom_segment(
  data = dat2,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 0.8
) + geom_segment(
  data = dat3,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 0.8
) + geom_segment(
  data = dat4,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 0.8
) + geom_segment(
  data = dat5,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 0.8, linetype ="dotted"
) + geom_segment(
  data = dat6,
  mapping = aes(x = x[1], xend = x[2], y = y[1], yend = y[2]),
  color = "orange", linewidth = 0.8, linetype ="dotted"
) + geom_segment(
  mapping = aes(x = 5, xend = 7, y = 3.3, yend = 3.3),
  color = arrow_color, linewidth = 0.6,
  arrow = arrow(length = unit(0.1, "inches"), ends = "both", angle = 40)
) +
ggtitle("Difference-in-differences")
```

Let's look at the different slopes separately first, i.e. the slopes of `Sepal.Length` by levels of `Species`:

```{r}
hypothesis_test(m, c("Sepal.Length", "Species"), test = NULL)
```

```{r echo=FALSE}
ht6 <- hypothesis_test(m, c("Sepal.Length", "Species"), test = NULL)
```

The first difference of slopes we're interested in is the one between `setosa` (`r round(ht6$Slope[1], 2)`) and `versicolor` (`r round(ht6$Slope[2], 2)`), i.e. `b1 - b2` (=`r round(ht6$Slope[1] - ht6$Slope[2], 2)`). The second difference is between levels `setosa` (`r round(ht6$Slope[1], 2)`) and `virginica` (`r round(ht6$Slope[3], 2)`), which is `b1 - b3` (=`r round(ht6$Slope[1] - ht6$Slope[3], 2)`). We test the null hypothesis that `(b1 - b2) = (b1 - b3)`.

```{r}
hypothesis_test(m, c("Sepal.Length", "Species"), test = "(b1 - b2) = (b1 - b3)")
```

```{r echo=FALSE}
ht7 <- hypothesis_test(m, c("Sepal.Length", "Species"), test = "(b1 - b2) = (b1 - b3)")
```

The difference between the two differences is `r round(ht7$Contrast[1], 2)` and not statistically significant (p = `r round(ht7$p.value[1], 3)`).

## Is the linear trend of `Sepal.Length` significant at different values of another numeric predictor?

When we have two numeric terms in an interaction, the comparison becomes more difficult, because we have to find *meaningful* (or *representative*) values for the moderator, at which the associations between the predictor and outcome are tested. We no longer have distinct categories for the moderator variable.

### Spotlight analysis, floodlight analysis and Johnson-Neyman intervals

The last examples show interactions between two numeric predictors. In case of interaction terms, adjusted predictions are usually shown at _representative values_. If a _numeric_ variable is specified as second or third interaction term, representative values (see `values_at()`) are typically mean +/- SD. This is sometimes also called "spotlight analysis" (_Spiller et al. 2013_).

In the next example, we have `Petal.Width` as second interaction term, thus we see the predicted values of `Sepal.Width` (our outcome) for `Petal.Length` at three different, representative values of `Petal.Width`: Mean (`r round(mean(iris$Petal.Width), 2)`), 1 SD above the mean (`r round(mean(iris$Petal.Width) + sd(iris$Petal.Width), 2)`) and 1 SD below the mean (`r round(mean(iris$Petal.Width) - sd(iris$Petal.Width), 2)`).

### Predictions

```{r}
m <- lm(Sepal.Width ~ Petal.Length * Petal.Width, data = iris)
pred <- ggpredict(m, c("Petal.Length", "Petal.Width"))
plot(pred)
```

For `hypothesis_test()`, these three values (mean, +1 SD and -1 SD) work in the same way as if `Petal.Width` was a categorical predictor with three levels.

First, we want to see at which value of `Petal.Width` the slopes of `Petal.Length` are significant. We do no pairwise comparison here, hence we set `test = NULL`.

```{r}
hypothesis_test(pred, test = NULL)
# same as:
# hypothesis_test(m, c("Petal.Length", "Petal.Width"), test = NULL)
```

### Pairwise comparisons

The results of the pairwise comparison are shown below. These tell us that all linear trends (slopes) are significantly different from each other, i.e. the slope of the green line is significantly different from the slope of the red line, and so on.

```{r}
hypothesis_test(pred)
```

### Floodlight analysis and Johnson-Neyman intervals

Another way to handle models with two numeric variables in an interaction is to use so-called floodlight analysis, a spotlight analysis for all values of the moderator variable, which is implemented in the `johnson_neyman()` function that creates Johnson-Neyman intervals. These intervals indicate the values of the moderator at which the slope of the predictor is significant (cf. _Johnson et al. 1950, McCabe et al. 2018_).

Let's look at an example. We first plot the predicted values of `Income` for `Murder` at nine different values of `Illiteracy` (there are no more colors in the default palette to show more lines).

```{r}
states <- as.data.frame(state.x77)
states$HSGrad <- states$`HS Grad`
m_mod <- lm(Income ~ HSGrad + Murder * Illiteracy, data = states)

myfun <- seq(0.5, 3, length.out = 9)
pr <- ggpredict(m_mod, c("Murder", "Illiteracy [myfun]"))
plot(pr)
```

It's difficult to say at which values from `Illiteracy`, the association between `Murder` and `Income` might be statistically signifiant. We still can use `hypothesis_test()`:

```{r}
hypothesis_test(pr, test = NULL)
```

As can be seen, the results might indicate that at the lower and upper tails of `Illiteracy`, i.e. when `Illiteracy` is roughly smaller than `0.8` or larger than `2.6`, the association between `Murder` and `Income` is statistically signifiant.

However, this test can be simplified using the `johnson_neyman()` function:

```{r eval=FALSE}
johnson_neyman(pr)
#> The association between `Murder` and `Income` is positive for values of
#> `Illiteracy` lower than 0.79 and negative for values higher than 2.67.
#> Inside the interval of [0.79, 2.67], there were no clear associations.
```

Furthermore, it is possible to create a spotlight-plot.

```{r message=FALSE}
plot(johnson_neyman(pr))
```

To avoid misleading interpretations of the plot, we speak of "positive" and "negative" associations, respectively, or "no clear" associations (instead of "significant" or "non-significant"). This should prevent considering a non-significant range of values of the moderator as "accepting the null hypothesis".

# Contrasts and comparisons for GLM - logistic regression example

Lastly, we show an example for non-Gaussian models. For GLM models with (non-Gaussian) link-functions, `ggpredict()` always returns predcted values on the *response* scale. For example, predicted values for logistic regression models are shown as *probabilities*.

Let's look at a simple example

```{r}
set.seed(1234)
dat <- data.frame(
  outcome = rbinom(n = 100, size = 1, prob = 0.35),
  x1 = as.factor(sample(1:3, size = 100, TRUE, prob = c(0.5, 0.2, 0.3))),
  x2 = rnorm(n = 100, mean = 10, sd = 7)
)

m <- glm(outcome ~ x1 + x2, data = dat, family = binomial())
ggpredict(m, "x1")
```

## Contrasts and comparisons for categorical focal terms

Contrasts or comparisons - like predictions (see above) - are by default on the *response* scale, i.e. they're represented as difference between probabilities (in percentage points).

```{r message=TRUE}
p <- ggpredict(m, "x1")
hypothesis_test(p)
```

```{r echo=FALSE}
ht8 <- hypothesis_test(p)
```

The difference between the predicted probability of `x1 = 1` (`r sprintf("%.1f%%", 100 * p$predicted[1])`) and `x1 = 2` (`r sprintf("%.1f%%", 100 * p$predicted[2])`) is roughly `r sprintf("%.1f%%", 100 * ht8$Contrast[1])` points. This difference is not statistically significant (p = `r round(ht8$p.value[1], 3)`).

The `scale` argument in `hypothesis_test()` can be used to return contrasts or comparisons on a differen scale. For example, to transform contrasts to _odds ratios_, we can use `scale = "exp"`.

```{r message=TRUE}
hypothesis_test(p, scale = "exp")
```

Contrasts or comparisons can also be represented on the link-scale, in this case as _log-odds_. To do so, use `scale = "link"`.

```{r message=TRUE}
hypothesis_test(p, scale = "link")
```

## Contrasts and comparisons for numerical focal terms

For numeric focal variables, where the slopes (linear trends) are estimated, transformed scales (like `scale = "exp"`) are not supported. However, `scale = "link"` can be used to return untransformed contrasts or comparisons on the link-scale.

```{r message=TRUE}
hypothesis_test(m, "x2", scale = "link")
```

Be aware whether and which back-transformation to use, as it affects the resulting p-values. A detailed overview of transformations can be found [in this vignette](https://vincentarelbundock.github.io/marginaleffects/articles/comparisons.html).

# Conclusion

Thanks to the great *marginaleffects* package, it is now possible to have a powerful function in *ggeffects* that allows to perform the next logical step after calculating adjusted predictions and to conduct hypothesis tests for contrasts and pairwise comparisons.

While the current implementation in `hypothesis_test()` already covers many common use cases for testing contrasts and pairwise comparison, there still might be the need for more sophisticated comparisons. In this case, I recommend using the [*marginaleffects*](https://vincentarelbundock.github.io/marginaleffects/) package directly. Some further related recommended readings are the vignettes about [Comparisons](https://vincentarelbundock.github.io/marginaleffects/articles/comparisons.html) or [Hypothesis Tests, Equivalence Tests, and Custom Contrasts](https://vincentarelbundock.github.io/marginaleffects/articles/hypothesis.html).

# References

Johnson, P.O. & Fay, L.C. (1950). The Johnson-Neyman technique, its theory and application. Psychometrika, 15, 349-367. doi: 10.1007/BF02288864

McCabe CJ, Kim DS, King KM. (2018). Improving Present Practices in the Visual Display of Interactions. Advances in Methods and Practices in Psychological Science, 1(2):147-165. doi:10.1177/2515245917746792

Spiller, S. A., Fitzsimons, G. J., Lynch, J. G., & McClelland, G. H. (2013). Spotlights, Floodlights, and the Magic Number Zero: Simple Effects Tests in Moderated Regression. Journal of Marketing Research, 50(2), 277–288. doi:10.1509/jmr.12.0420